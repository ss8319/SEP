## Setup Instructions

1. **Install Conda**: Ensure Anaconda or Miniconda is installed. 

2. **Create and Activate Environment**:
   ```bash
   conda env create -f environment.yml
   conda activate [env-name]

Replace [env-name] with the name of the environment specified at the top of your environment.yml file.


# Overview of the Main Steps in the Paper
[Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs](https://arxiv.org/abs/2406.15927).

1. Semantic Entropy Probes (SEPs) Introduction and Justification
Objective: SEPs are introduced as tools to quantify semantic entropy directly from the hidden states of language models without the need for extensive sampling, thereby reducing computational overhead.
Rationale: Unlike traditional entropy calculation methods that require sampling multiple responses, SEPs utilize hidden states from a single model generation, simplifying the process and reducing costs, especially during test time.
2. Data Preparation and Feature Extraction
Input Data: Utilizes queries from QA datasets like TriviaQA, SQuAD, BioASQ, and NQ Open.
Feature Extraction: Hidden states are extracted from specified token positions (TBG and SLT) across all layers of the model to investigate which layers best capture semantic entropy.
3. Semantic Entropy Calculation and Binarization
Calculation: Semantic entropy (HSE) is calculated for responses generated by the model based on the variability and distribution of the responses.
Binarization: HSE scores are converted into binary labels, defining them as high or low based on an optimal threshold. This thresholding helps in simplifying the classification problem for logistic regression.
4. Training of Semantic Entropy Probes
Model Setup: SEPs are implemented using logistic regression models trained on the hidden states of the language models to predict the binarized semantic entropy.
Training Data: The training set comprises pairs of hidden states and corresponding binary labels of semantic entropy.
5. Evaluation Strategy
Performance Metrics: SEPs are evaluated based on their ability to accurately predict semantic entropy and detect model hallucinations using metrics like AUROC.
Comparative Analysis: The performance of SEPs is compared against various baselines including model correctness labels, naive entropy, log likelihood, and the p(True) method to establish their efficacy and reliability.
6. Practical Implementation and Testing
Testing: SEPs are tested across different tasks, model configurations, token indices, and layers to assess their generalizability and effectiveness in real-world scenarios.
Cost-Efficiency: The methodology emphasizes the reduced computational cost of SEPs, making them suitable for scenarios where quick and reliable estimates of semantic uncertainty are needed without extensive computational resources.
7. Discussion and Further Insights
Insights: The paper discusses the inherent advantages of using hidden states to predict semantic properties like entropy and truthfulness, positing that these properties are encoded in the hidden states and can be extracted more efficiently than through traditional methods.
Future Work: Suggestions for extending the utility of SEPs, potential improvements in probe design, and applications in other areas of NLP.