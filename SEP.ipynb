{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/sep/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Load and Deduplicate Questions\n",
    "def deduplicate_questions(questions):\n",
    "    seen = set()\n",
    "    unique_questions = []\n",
    "    for question in questions:\n",
    "        if question not in seen:\n",
    "            seen.add(question)\n",
    "            unique_questions.append(question)\n",
    "    return unique_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Responses\n",
    "def generate_responses(prompt, num_samples=4, max_new_tokens=50):\n",
    "    responses = []\n",
    "    for _ in range(num_samples):\n",
    "        outputs = response_generator(prompt, max_length=max_new_tokens, num_return_sequences=1, do_sample=True)\n",
    "        if outputs and 'generated_text' in outputs[0]:\n",
    "            responses.append(outputs[0]['generated_text'].strip())\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Semantic Clustering\n",
    "\n",
    "def check_entailment(text1, text2):\n",
    "    inputs = tokenizer(text1, text2, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = nli_model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    return torch.argmax(probs, dim=1).item() == 0\n",
    "\n",
    "def semantic_clustering(responses):\n",
    "    clusters = []\n",
    "    for response in responses:\n",
    "        found = False\n",
    "        for cluster in clusters:\n",
    "            if all(check_entailment(response, member) for member in cluster['responses']):\n",
    "                cluster['responses'].append(response)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            clusters.append({'responses': [response]})\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Step 4: Calculate Entropy\n",
    "def calculate_entropy(clusters):\n",
    "    # Debug: Check the structure of clusters\n",
    "    if not all(isinstance(cluster, dict) and 'responses' in cluster for cluster in clusters):\n",
    "        raise ValueError(\"Each cluster should be a dictionary with a 'responses' key.\")\n",
    "\n",
    "    total_responses = sum(len(cluster['responses']) for cluster in clusters)\n",
    "    cluster_probs = np.array([len(cluster['responses']) / total_responses for cluster in clusters])\n",
    "    return -np.sum(cluster_probs * np.log(cluster_probs))\n",
    "# Step 5: Optimal Threshold\n",
    "def find_optimal_threshold(entropies):\n",
    "    sorted_entropies = np.sort(entropies)\n",
    "    best_threshold, min_error = None, float('inf')\n",
    "    for i in range(1, len(sorted_entropies)):\n",
    "        threshold = (sorted_entropies[i - 1] + sorted_entropies[i]) / 2\n",
    "        low_group, high_group = [e for e in entropies if e < threshold], [e for e in entropies if e >= threshold]\n",
    "        mse_low, mse_high = np.var(low_group), np.var(high_group)\n",
    "        total_mse = mse_low + mse_high\n",
    "        if total_mse < min_error:\n",
    "            min_error, best_threshold = total_mse, threshold\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "# Step 6: Extract Hidden States for SEP Training \n",
    "def extract_hidden_states(model, tokenizer, texts, position='TBG'):\n",
    "    \"\"\"\n",
    "    Extract hidden states from a given model at specified token positions.\n",
    "    \n",
    "    Args:\n",
    "    - model (AutoModelForCausalLM): Pre-trained model from Hugging Face.\n",
    "    - tokenizer (AutoTokenizer): Corresponding tokenizer for the model.\n",
    "    - texts (list of str): Input texts to process.\n",
    "    - position (str): Position to extract hidden states from ('TBG' or 'SLT').\n",
    "    \n",
    "    Returns:\n",
    "    - dict of torch.Tensor: Hidden states for each layer at specified position.\n",
    "    \"\"\"\n",
    "    model.eval()  # Ensure the model is in evaluation mode.\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    hidden_states = outputs.hidden_states\n",
    "    if position == 'TBG':\n",
    "        # Token Before Generating: last token of the input (excluding padding)\n",
    "        indices = (inputs['input_ids'] != tokenizer.pad_token_id).sum(dim=1) - 1\n",
    "    elif position == 'SLT':\n",
    "        # Second Last Token: just before the end-of-sequence token\n",
    "        indices = inputs['input_ids'].shape[1] - 2\n",
    "    \n",
    "    # Extract hidden states for the specified position across all layers\n",
    "    position_states = {f'Layer_{i}': hidden_states[i][:, indices, :] for i in range(len(hidden_states))}\n",
    "    \n",
    "    return position_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-base')\n",
    "text_model = AutoModelForCausalLM.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "# Step 1: Load and Deduplicate Questions\n",
    "# Load and deduplicate questions\n",
    "dataset = load_dataset(\"kroshan/BioASQ\")\n",
    "questions = dataset['train']['question']\n",
    "unique_questions = deduplicate_questions(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Responses\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "response_generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"cuda\",#cpu\n",
    ")\n",
    "# responses = [generate_responses(question) for question in unique_questions[:3]]\n",
    "\n",
    "responses = [generate_responses(question) for question in unique_questions[:10]]## For a minimal example set to 10 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save responses\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path where you want to save the CSV file\n",
    "file_path = 'questions_and_responses.csv'\n",
    "\n",
    "# Save the questions and responses to a CSV file\n",
    "with open(file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Question', 'Response'])  # Write the header\n",
    "    writer.writerows(zip(unique_questions[:5], responses))  # Write each question and response\n",
    "\n",
    "print(f\"Questions and responses saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Uncomment to load saved responses\n",
    "\n",
    "# # Define the file path where you want to save the CSV file\n",
    "# file_path = 'questions_and_responses.csv'\n",
    "\n",
    "# ques_and_response = pd.read_csv(file_path)\n",
    "# responses = ques_and_response['Response']\n",
    "# responses = responses.tolist()\n",
    "# responses = [item.strip(\"[]\").replace(\"'\", \"\").split(\", \") for item in responses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic clustering\n",
    "clustered_responses = [semantic_clustering(response_set) for response_set in responses]\n",
    "\n",
    "# Calculate entropy for each cluster\n",
    "entropies = [calculate_entropy(cluster) for cluster in clustered_responses]\n",
    "\n",
    "# Find optimal threshold for entropy\n",
    "optimal_threshold = find_optimal_threshold(entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hidden_states(hidden_states):\n",
    "    # Flatten the tensor and convert to numpy array\n",
    "    return np.concatenate([hidden_states[layer].cpu().numpy().flatten() for layer in hidden_states], axis=0)\n",
    "\n",
    "# Collecting and preparing training data\n",
    "training_data = []\n",
    "labels = []\n",
    "\n",
    "for text in unique_questions[:10]:\n",
    "    hidden_states_tbg = extract_hidden_states(text_model, tokenizer, [text], 'TBG')\n",
    "    hidden_states_slt = extract_hidden_states(text_model, tokenizer, [text], 'SLT')\n",
    "\n",
    "    # Flatten the hidden states for TBG and SLT\n",
    "    features_tbg = flatten_hidden_states(hidden_states_tbg)\n",
    "    features_slt = flatten_hidden_states(hidden_states_slt)\n",
    "\n",
    "    # Assume generate_responses and calculate_entropy are defined\n",
    "    # responses = generate_responses(text)\n",
    "    clusters = semantic_clustering(responses)\n",
    "    entropy = calculate_entropy(clusters)\n",
    "    \n",
    "    # Binarize entropy based on the previously found optimal threshold\n",
    "    label = 1 if entropy > optimal_threshold else 0\n",
    "\n",
    "    # Append features and labels to the training data\n",
    "    training_data.append(features_tbg)\n",
    "    training_data.append(features_slt)\n",
    "    labels.extend([label, label])\n",
    "\n",
    "# Convert list to numpy arrays for training\n",
    "X = np.array(training_data)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict and plot confusion matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Plotting semantic entropy vs. hidden state values\n",
    "plt.scatter(X_train[:, 0], y_train, c='blue', label='Training Data')\n",
    "plt.scatter(X_test[:, 0], y_pred, c='red', label='Predicted Data')\n",
    "plt.xlabel('Hidden State Value')\n",
    "plt.ylabel('Semantic Entropy (Binarized)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
